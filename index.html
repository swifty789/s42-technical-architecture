<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>S42 Labs — Technical Architecture Diagrams</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter+Tight:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
    <style>
        :root {
            --brand: #6B2FA0;
            --brand-dark: #5A2590;
            --accent: #E03E8E;
            --electric: #3B4DFF;
            --grey-50: #FAFAFA;
            --grey-100: #F5F5F5;
            --grey-200: #E5E5E5;
            --grey-400: #AAAAAA;
            --grey-600: #666666;
            --grey-800: #333333;
            --grey-900: #1A1A1A;
            --dark-bg: #0A0A14;
            --white: #FFFFFF;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: 'Inter Tight', system-ui, sans-serif;
            background: var(--grey-100);
            color: var(--grey-800);
            line-height: 1.7;
        }

        .container { max-width: 1200px; margin: 0 auto; padding: 0 24px; }
        @media (min-width: 768px) { .container { padding: 0 40px; } }
        @media (min-width: 1024px) { .container { padding: 0 48px; } }

        /* ── Header ── */
        header { background: var(--dark-bg); }
        .gradient-bar { height: 4px; background: linear-gradient(90deg, var(--electric) 0%, var(--brand) 50%, var(--accent) 100%); }
        .header-inner { padding: 36px 0 48px; display: flex; flex-direction: column; gap: 24px; }
        @media (min-width: 768px) {
            .header-inner { flex-direction: row; align-items: flex-end; justify-content: space-between; }
        }
        .logo { height: 32px; margin-bottom: 20px; }
        @media (min-width: 768px) { .logo { height: 36px; } }
        header h1 { font-size: 32px; font-weight: 800; color: var(--white); line-height: 1.1; letter-spacing: -0.5px; }
        @media (min-width: 768px) { header h1 { font-size: 40px; } }
        @media (min-width: 1024px) { header h1 { font-size: 48px; } }
        .header-subtitle { color: rgba(255,255,255,0.5); font-size: 15px; margin-top: 8px; }
        .header-meta { font-size: 13px; color: rgba(255,255,255,0.35); line-height: 1.8; flex-shrink: 0; }
        @media (min-width: 768px) { .header-meta { text-align: right; } }

        /* ── Cards ── */
        .card {
            background: var(--white);
            border: 1px solid var(--grey-200);
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 20px;
        }
        @media (min-width: 768px) { .card { padding: 36px; } }
        .card h2 { font-size: 20px; font-weight: 700; color: var(--grey-900); margin-bottom: 12px; }
        .card h3 { font-size: 16px; font-weight: 600; color: var(--grey-900); margin: 20px 0 6px; }
        .card p { font-size: 14px; color: var(--grey-600); margin-bottom: 10px; }
        .card p strong { color: var(--grey-800); }
        .card ul { padding-left: 20px; margin-bottom: 12px; }
        .card li { font-size: 14px; color: var(--grey-600); margin: 4px 0; }
        .card li strong { color: var(--grey-800); }

        /* ── TOC ── */
        .toc { border-left: 4px solid var(--brand); }
        .toc h2 { font-size: 14px; text-transform: uppercase; letter-spacing: 0.05em; margin-bottom: 12px; }
        .toc ol { padding-left: 20px; }
        .toc li { margin: 8px 0; }
        .toc a { color: var(--brand); text-decoration: none; font-size: 14px; font-weight: 500; }
        .toc a:hover { text-decoration: underline; }

        /* ── Section headings ── */
        .section-heading { display: flex; align-items: center; margin-bottom: 16px; }
        .section-number {
            display: inline-flex; align-items: center; justify-content: center;
            width: 32px; height: 32px; border-radius: 50%;
            background: linear-gradient(135deg, var(--electric), var(--brand), var(--accent));
            color: var(--white); font-size: 14px; font-weight: 700;
            margin-right: 12px; flex-shrink: 0;
        }
        .section-heading h2 { font-size: 20px; }
        @media (min-width: 768px) { .section-heading h2 { font-size: 24px; } }

        /* ── Mermaid diagrams ── */
        .diagram-wrapper {
            margin: 24px 0; padding: 16px; border-radius: 12px;
            border: 1px solid var(--grey-200); background: var(--grey-100);
            overflow-x: auto; cursor: pointer;
            transition: border-color 0.2s, box-shadow 0.2s;
            position: relative;
        }
        @media (min-width: 768px) { .diagram-wrapper { padding: 24px; } }
        .diagram-wrapper:hover { border-color: rgba(107,47,160,0.3); box-shadow: 0 4px 12px rgba(0,0,0,0.08); }
        .diagram-wrapper .expand-hint {
            position: absolute; top: 12px; right: 12px;
            padding: 5px 10px; border-radius: 8px;
            background: rgba(255,255,255,0.8); backdrop-filter: blur(8px);
            border: 1px solid var(--grey-200);
            font-size: 12px; color: var(--grey-400); font-weight: 500;
            opacity: 0; transition: opacity 0.2s; pointer-events: none; z-index: 10;
        }
        .diagram-wrapper:hover .expand-hint { opacity: 1; }
        .mermaid svg { max-width: 100%; height: auto; }

        /* ── Code ── */
        code {
            background: var(--grey-100); border: 1px solid var(--grey-200);
            padding: 2px 6px; border-radius: 4px; font-size: 13px;
            font-family: 'SF Mono', 'Fira Code', monospace; color: var(--brand);
        }

        /* ── Tables ── */
        .table-wrap { overflow-x: auto; border-radius: 8px; border: 1px solid var(--grey-200); margin: 12px 0; }
        table { border-collapse: collapse; width: 100%; font-size: 14px; }
        th { background: var(--dark-bg); color: var(--white); padding: 10px 16px; text-align: left; font-weight: 600; font-size: 12px; text-transform: uppercase; letter-spacing: 0.03em; }
        td { border-top: 1px solid var(--grey-200); padding: 10px 16px; color: var(--grey-800); }
        tr:nth-child(even) { background: var(--grey-100); }
        .badge { display: inline-block; padding: 2px 8px; border-radius: 4px; font-size: 12px; font-weight: 500; }
        .badge-api { background: rgba(107,47,160,0.1); color: var(--brand); }
        .badge-ui { background: rgba(224,62,142,0.1); color: var(--accent); }
        .badge-db { background: rgba(59,77,255,0.1); color: var(--electric); }
        .badge-other { background: var(--grey-200); color: var(--grey-600); }

        /* ── Modal ── */
        .modal-overlay {
            display: none; position: fixed; inset: 0; z-index: 100;
            align-items: center; justify-content: center;
            background: rgba(0,0,0,0.7); backdrop-filter: blur(4px);
        }
        .modal-overlay.active { display: flex; }
        .modal-content {
            width: 95vw; max-width: 1600px; height: 90vh;
            background: var(--white); border-radius: 16px;
            box-shadow: 0 24px 64px rgba(0,0,0,0.3);
            display: flex; flex-direction: column; overflow: hidden;
        }
        .modal-bar {
            display: flex; align-items: center; justify-content: space-between;
            padding: 14px 20px; border-bottom: 1px solid var(--grey-200);
            background: var(--grey-100); flex-shrink: 0;
        }
        @media (min-width: 768px) { .modal-bar { padding: 14px 28px; } }
        .modal-bar span { font-size: 13px; color: var(--grey-400); font-weight: 500; }
        .modal-close {
            display: flex; align-items: center; gap: 6px;
            padding: 6px 14px; border-radius: 8px; border: none;
            background: transparent; font-size: 13px; font-weight: 500;
            color: var(--grey-600); cursor: pointer; font-family: inherit;
        }
        .modal-close:hover { background: var(--grey-200); color: var(--grey-900); }
        .modal-body { flex: 1; overflow: auto; padding: 24px; display: flex; align-items: flex-start; justify-content: center; }
        @media (min-width: 768px) { .modal-body { padding: 40px; } }
        .modal-body svg { max-width: 100%; height: auto; }

        /* ── Footer ── */
        footer { text-align: center; padding: 28px 0; font-size: 13px; color: var(--grey-400); }
        footer a { color: var(--brand); text-decoration: none; }
        footer a:hover { text-decoration: underline; }

        /* ── Print ── */
        @media print {
            body { background: var(--white); }
            .card { border: none; box-shadow: none; page-break-inside: avoid; }
            header { -webkit-print-color-adjust: exact; print-color-adjust: exact; }
            .diagram-wrapper { background: var(--white); border: 1px solid var(--grey-200); }
            .expand-hint, .modal-overlay { display: none !important; }
        }

        /* ── Content section ── */
        .content { padding: 32px 0; }
        .section-card { margin-bottom: 24px; }
        .section-card .body p { font-size: 14px; color: var(--grey-600); margin-bottom: 10px; line-height: 1.7; }
        .section-card .body p strong { color: var(--grey-800); }
        .sub-heading { font-size: 17px; font-weight: 600; color: var(--grey-900); margin: 32px 0 8px; }
    </style>
</head>
<body>
    <header>
        <div class="gradient-bar"></div>
        <div class="container">
            <div class="header-inner">
                <div>
                    <svg class="logo" viewBox="0 0 160 36" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <circle cx="18" cy="18" r="16" stroke="white" stroke-width="2"/>
                        <circle cx="18" cy="18" r="4" fill="white"/>
                        <line x1="18" y1="2" x2="18" y2="10" stroke="white" stroke-width="2"/>
                        <line x1="18" y1="26" x2="18" y2="34" stroke="white" stroke-width="2"/>
                        <line x1="2" y1="18" x2="10" y2="18" stroke="white" stroke-width="2"/>
                        <line x1="26" y1="18" x2="34" y2="18" stroke="white" stroke-width="2"/>
                        <text x="44" y="25" fill="white" font-family="Inter Tight, sans-serif" font-weight="800" font-size="22">S42 Labs</text>
                    </svg>
                    <h1>Technical Architecture Diagrams</h1>
                    <p class="header-subtitle">Signal OS &bull; Memory RAG &bull; Infrastructure</p>
                </div>
                <div class="header-meta">
                    Last Updated: 2026-02-11<br>
                    Author: Jason Liew<br>
                    Source: signal-core-memory-knowledge<br>
                    Status: Living document
                </div>
            </div>
        </div>
    </header>

    <div class="container content">

        <!-- About -->
        <div class="card">
            <h2>About This Document</h2>
            <p>Signal OS is the platform that sits behind every AI product S42 Labs builds. Think of it as the engine room — it handles everything from understanding a user's question, to searching through company documents for relevant information, to generating an intelligent answer with proper source references.</p>
            <p>This document maps out how that engine room is constructed. Every diagram here is drawn directly from the production codebase (the code that's actually running), not from aspirational plans or slide decks.</p>
            <h3>Who Is This For?</h3>
            <ul>
                <li><strong>Technical leads and architects</strong> evaluating S42 Labs' platform — use this as a credentials reference to understand the depth of the RAG pipeline, memory architecture, and multi-provider LLM strategy.</li>
                <li><strong>Product and commercial stakeholders</strong> who want to understand what Signal OS actually does, how data moves through the system, and why the architecture is built the way it is — without needing to read code.</li>
                <li><strong>Internal team members</strong> needing a single-page reference for how data flows through the system, which ports to use, and which LLM handles which task.</li>
            </ul>
            <h3>How To Use This Document</h3>
            <p>The diagrams are ordered from big picture down to specific subsystems. <strong>If you're new to the platform</strong>, start with Diagram 1 (the full architecture — how everything connects) and Diagram 5 (the end-to-end data flow — what happens step by step when a document is uploaded or a question is asked). Those two give you the full story. The remaining diagrams zoom into individual pieces for when you want more detail.</p>
            <p>You can click any diagram to expand it to full-screen for a closer look.</p>
            <h3>Monorepo Structure</h3>
            <p>The platform is organised as a <strong>monorepo</strong> — a single code repository that contains three separate services, each responsible for a distinct part of the system. Here's what each one does:</p>
            <div class="table-wrap">
                <table>
                    <tr><th>Package</th><th>Service</th><th>Port</th><th>Primary Database</th><th>Description</th></tr>
                    <tr><td><code>packages/memory/</code></td><td>Memory Service</td><td>:8000</td><td>MongoDB</td><td>The central orchestrator. Receives every user message, gathers relevant context (user profile, company data, past conversations, calendar, tasks), and sends it all to the AI for a personalised response.</td></tr>
                    <tr><td><code>packages/doc-ingestion/</code></td><td>Doc-Ingestion Service</td><td>:8009</td><td>PostgreSQL</td><td>Processes uploaded documents (PDFs, Word files, spreadsheets, etc.) through an 11-step AI pipeline that extracts, validates, and chunks content so it can be searched later.</td></tr>
                    <tr><td><code>packages/rag-engine/</code></td><td>RAG Engine</td><td>:8008</td><td>Pinecone (external)</td><td>The search and answer engine. When a user asks a question, this service finds the most relevant document passages, ranks them by relevance, and generates an answer with source citations.</td></tr>
                </table>
            </div>
            <p>A <code>shared/</code> directory contains cross-package utilities, and <code>docker-compose.yml</code> at the root orchestrates all services and infrastructure.</p>
        </div>

        <!-- TOC -->
        <nav class="card toc">
            <h2>Contents</h2>
            <ol>
                <li><a href="#signal-os">Signal OS — Full Memory RAG Architecture</a></li>
                <li><a href="#context-assembly">Memory Service — Context Assembly</a></li>
                <li><a href="#doc-ingestion">Doc-Ingestion — 11-Node LangGraph Pipeline</a></li>
                <li><a href="#rag-engine">RAG Engine — 5-Node Query Pipeline</a></li>
                <li><a href="#data-flow">Full Data Flow — Upload to Answer</a></li>
                <li><a href="#infrastructure">Infrastructure &amp; Deployment</a></li>
                <li><a href="#llm-routing">LLM Model Routing</a></li>
            </ol>
        </nav>

        <!-- 1 -->
        <section id="signal-os" class="card section-card">
            <div class="section-heading"><span class="section-number">1</span><h2>Signal OS — Full Memory RAG Architecture</h2></div>
            <div class="body">
                <p>This is the big picture. It shows the entire platform — every service, every database, and every external AI provider — on a single page. If you only look at one diagram, make it this one.</p>
                <p>At the centre is the <strong>Memory Service</strong> — the orchestrator. When a user sends a message, it gathers everything the AI needs to respond well: who the user is, what company they work for, what they've asked before, what's on their calendar, and any relevant documents. It then passes all of that context to the AI model for a personalised response.</p>
                <p>The <strong>Doc-Ingestion Service</strong> handles the other side of the equation: getting documents <em>into</em> the system. When someone uploads a PDF or Word document, this service processes it through an 11-step pipeline that extracts content, validates quality, and breaks it into searchable chunks.</p>
                <p>The <strong>RAG Engine</strong> sits between the two — it stores the processed document chunks and retrieves the most relevant ones when a user asks a question. RAG stands for "Retrieval-Augmented Generation", which simply means the AI doesn't guess from memory — it searches your actual documents first, then generates an answer based on what it finds.</p>
                <p>All three services communicate over a shared network, with databases (MongoDB, PostgreSQL, Redis, MinIO) running alongside them. External AI services — embedding models, vector search, language models — are called over the internet.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TB
    USER["User"] -->|Message| CHAT["Chat Interface"]
    CHAT -->|POST /api/chat| MEMORY
    subgraph MEMORY["MEMORY SERVICE :8000"]
        direction TB
        CHAT_API["Chat API"]
        DC["Data Controller<br/>(Context Assembly)"]
        ML["Manifest Loader<br/>(YAML configs)"]
        MEM0["Mem0<br/>(Learned Memories)"]
        LANGMEM["LangMem<br/>(Memory Extraction)"]
        CHAT_API --> DC
        DC --> ML
        DC --> MEM0
        DC --> LANGMEM
    end
    subgraph ADAPTORS["External Adaptors"]
        GMAIL["Gmail"]
        SLACK["Slack"]
        CAL["Calendar"]
        CU["ClickUp"]
    end
    MEMORY --> ADAPTORS
    subgraph DOCINGESTION["DOC-INGESTION SERVICE :8009"]
        direction TB
        UPLOAD_API["Upload API"]
        LANGGRAPH_DI["LangGraph Pipeline<br/>(11 Nodes)"]
        CELERY["Celery Worker"]
        UPLOAD_API --> LANGGRAPH_DI
        UPLOAD_API --> CELERY
    end
    subgraph RAGENGINE["RAG ENGINE :8008"]
        direction TB
        SEARCH_API["Search API"]
        ASK_API["Ask API"]
        INGEST_API["Ingest API"]
        LANGGRAPH_RAG["LangGraph Pipeline<br/>(5 Nodes)"]
        LLM_ROUTER["LLM Router<br/>(Multi-Provider)"]
        SEARCH_API --> LANGGRAPH_RAG
        ASK_API --> LANGGRAPH_RAG
        LANGGRAPH_RAG --> LLM_ROUTER
    end
    MEMORY -->|Search / Ask| RAGENGINE
    DOCINGESTION -->|Ingest Vectors| RAGENGINE
    subgraph INFRA["SHARED INFRASTRUCTURE"]
        MONGO[("MongoDB :27017<br/>Memory DB")]
        PG[("PostgreSQL :5433<br/>Doc Ingestion DB")]
        REDIS[("Redis :6380<br/>Task Queue")]
        MINIO[("MinIO :9000<br/>Object Storage")]
    end
    MEMORY --> MONGO
    DOCINGESTION --> PG
    DOCINGESTION --> REDIS
    DOCINGESTION --> MINIO
    RAGENGINE --> MINIO
    subgraph EXTERNAL["EXTERNAL AI SERVICES"]
        VOYAGE["Voyage AI<br/>Embeddings<br/>(voyage-multimodal-3.5)"]
        PINECONE["Pinecone<br/>Vector Store<br/>(Serverless)"]
        JINA["Jina AI<br/>Reranker<br/>(v2-base)"]
        LANGSMITH["LangSmith<br/>Observability"]
        ANTHROPIC["Anthropic<br/>Claude Sonnet 4.5"]
        GOOGLE["Google<br/>Gemini 3 Flash"]
        OPENAI_SVC["OpenAI<br/>GPT-5.2"]
    end
    RAGENGINE --> VOYAGE
    RAGENGINE --> PINECONE
    RAGENGINE --> JINA
    RAGENGINE --> LANGSMITH
    LLM_ROUTER --> ANTHROPIC
    LLM_ROUTER --> GOOGLE
    LLM_ROUTER --> OPENAI_SVC
    DOCINGESTION --> LANGSMITH
            </pre></div>
        </section>

        <!-- 2 -->
        <section id="context-assembly" class="card section-card">
            <div class="section-heading"><span class="section-number">2</span><h2>Memory Service — Context Assembly</h2></div>
            <div class="body">
                <p>The Memory Service is the brain of Signal OS. Every time a user sends a message, this service decides: <em>what does the AI need to know in order to give a genuinely useful response?</em></p>
                <p>The answer varies depending on the situation. An executive assistant AI needs calendar events and task lists. A sales coaching AI needs client documents and CRM data. A general assistant might need a bit of everything. Rather than loading all available context every time (which is slow and wasteful), the system uses <strong>Service Manifests</strong> — simple configuration files that define exactly what each AI service needs. The <strong>Data Controller</strong> (<code>app/services/data_controller.py</code>) reads the relevant manifest and fetches only the required context.</p>
                <p>The system also <strong>learns over time</strong>. After every interaction, a component called <strong>LangMem</strong> analyses the conversation for useful patterns — user preferences, communication style, recurring topics — and stores those learnings in a memory layer called <strong>Mem0</strong>. Next time, the AI has those insights available, so responses improve with use.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph LR
    MSG["User Message"] --> DC["Data Controller"]
    DC --> MANIFEST["Service Manifest<br/>(YAML)"]
    MANIFEST -->|user_profile| UP["User Profile<br/>(MongoDB)"]
    MANIFEST -->|tenant| TC["Tenant Config<br/>(MongoDB)"]
    MANIFEST -->|operational| OC["Operational<br/>Context"]
    MANIFEST -->|learned| M0["Mem0<br/>Memories"]
    MANIFEST -->|rag.enabled| RAG_Q["RAG Query"]
    UP --> CTX["Assembled<br/>Context"]
    TC --> CTX
    OC --> CTX
    M0 --> CTX
    RAG_Q --> CTX
    subgraph OPS["Operational Sources"]
        OC --> GMAIL_D["Gmail"]
        OC --> CAL_D["Google Calendar"]
        OC --> CU_D["ClickUp Tasks"]
        OC --> SLACK_D["Slack"]
    end
    CTX --> SYS["System Prompt<br/>+ Context"]
    SYS --> LLM["LLM"]
    LLM --> RESP["Response"]
    LLM --> LEARN["LangMem<br/>Extract Learnings"]
    LEARN --> M0
            </pre></div>
            <h3 class="sub-heading">Service Manifests</h3>
            <p>Manifests are simple configuration files (written in YAML) stored in <code>packages/memory/backend/app/manifests/</code>. Each one acts as a shopping list — it tells the Data Controller exactly which pieces of context to fetch. For example, the executive assistant manifest might request calendar events, tasks, and the user profile, while skipping client documents entirely. This keeps each AI service focused and efficient.</p>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TD
    subgraph MANIFESTS["Available Manifests"]
        DEF["default.yaml<br/>All context"]
        EXEC["exec_assistant.yaml<br/>Calendar + Tasks + Profile"]
        SALES["sales_coach.yaml<br/>Client docs + CRM data"]
    end
    MANIFESTS --> DC2["Data Controller"]
    DC2 -->|Selective Fetch| SOURCES["Only fetch<br/>what manifest requires"]
            </pre></div>
        </section>

        <!-- 3 -->
        <section id="doc-ingestion" class="card section-card">
            <div class="section-heading"><span class="section-number">3</span><h2>Doc-Ingestion — 11-Node LangGraph Pipeline</h2></div>
            <div class="body">
                <p>Before the AI can search a document, that document needs to be <em>understood</em> — parsed, analysed, validated, and broken into meaningful chunks. This is what the Doc-Ingestion Service does, and it's more involved than most people expect.</p>
                <p>The service runs each document through an <strong>11-step processing pipeline</strong> (<code>app/graph/graph.py</code>). Each step is a "node" in a graph, and the path a document takes depends on its content. A PDF with images is routed through a vision AI (Gemini 3 Flash) to describe those images. A spreadsheet with tables goes through table extraction. A plain text file skips straight to AI-powered metadata extraction. This conditional routing means each document gets the processing it actually needs.</p>
                <p>The pipeline is designed to be <strong>resilient</strong>. At every major stage, progress is saved to the database (a "checkpoint"). If something fails at step 7, the system can resume from step 6 rather than starting over. The final step is a <strong>manual approval gate</strong> — a human reviews the processed output before it enters the search index. This keeps quality high and prevents bad data from polluting the knowledge base.</p>
                <p>For bulk uploads, a background worker (Celery, backed by Redis) handles batch processing. All original files are stored in MinIO, an S3-compatible object store.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TD
    UPLOAD["Document Upload<br/>(PDF, DOCX, PPTX,<br/>XLSX, MD, TXT)"] --> META["1. METADATA<br/>File info, MIME type"]
    META --> PARSE["2. PARSE<br/>Format-specific parsers"]
    PARSE --> SIZE["3. SIZE CHECK<br/>Route by content"]
    SIZE -->|has_images| VISION["4. VISION<br/>Gemini 3 Flash"]
    SIZE -->|has_tables| TABLE["5. TABLE<br/>Table extraction"]
    SIZE -->|text_only| EXTRACT["6. EXTRACT<br/>AI metadata<br/>(Gemini / Claude)"]
    VISION -->|has_tables| TABLE
    VISION -->|no_tables| EXTRACT
    TABLE --> EXTRACT
    EXTRACT --> VALIDATE["7. VALIDATE<br/>Claude Sonnet 4.5"]
    VALIDATE --> FOCUS["8. FOCUS CHECK<br/>Relevance scoring"]
    FOCUS --> CHUNK["9. CHUNK<br/>Semantic chunking"]
    CHUNK --> TESTGEN["10. TEST GEN<br/>Q&amp;A pairs"]
    TESTGEN --> QUALITY["11. QUALITY<br/>Score 0-100"]
    QUALITY --> APPROVE{"Approve?"}
    APPROVE -->|Yes| INGEST["→ RAG Engine<br/>Embed + Pinecone"]
    APPROVE -->|No| ARCHIVE["Archive"]
    subgraph PARSERS["Format-Specific Parsers"]
        P_PDF["PDF"]
        P_DOCX["DOCX"]
        P_PPTX["PPTX"]
        P_XLSX["Excel"]
        P_MD["Markdown"]
        P_TXT["Text"]
    end
    PARSE --> PARSERS
            </pre></div>
            <h3 class="sub-heading">Checkpoint System</h3>
            <p>Processing a large document can take several minutes. If something fails midway (a network timeout, an AI model rate limit), you don't want to start over from scratch. The checkpoint system saves progress to PostgreSQL at six key stages. If a failure occurs, processing resumes from the last successful checkpoint — saving time and compute cost.</p>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph LR
    subgraph CHECKPOINTS["Checkpoint Stages (PostgreSQL)"]
        CP1["CP1: Metadata"]
        CP2["CP2: Parsing"]
        CP3["CP3: Extraction"]
        CP4["CP4: Validation"]
        CP5["CP5: Chunking"]
        CP6["CP6: Completed"]
    end
    CP1 --> CP2 --> CP3 --> CP4 --> CP5 --> CP6
    FAIL["Failure at any stage"] -.->|Resume from<br/>last checkpoint| CP3
            </pre></div>
        </section>

        <!-- 4 -->
        <section id="rag-engine" class="card section-card">
            <div class="section-heading"><span class="section-number">4</span><h2>RAG Engine — 5-Node Query Pipeline</h2></div>
            <div class="body">
                <p>When a user asks a question, this is the service that finds the answer. The RAG Engine runs a <strong>5-step pipeline</strong> that takes a question and returns an answer with source citations — similar to how a researcher would search through documents, pick the most relevant passages, and synthesise a response.</p>
                <p>Here's what each step does:</p>
                <ul>
                    <li><strong>Embed:</strong> Converts the question into a numerical representation (a "vector") using Voyage AI. This allows the system to search by meaning, not just keyword matching.</li>
                    <li><strong>Search:</strong> Queries the Pinecone vector database using two strategies simultaneously — meaning-based search (dense) and keyword-based search (sparse/BM25) — for the best of both worlds.</li>
                    <li><strong>Rerank:</strong> Takes the initial search results and re-scores them using Jina AI, a specialist reranking model, to push the most truly relevant results to the top.</li>
                    <li><strong>Compress:</strong> Trims the selected passages down to fit within the AI model's context window (there's a limit to how much text you can send to an LLM at once).</li>
                    <li><strong>Generate:</strong> Sends the compressed context to the appropriate language model, which produces the final answer along with source references.</li>
                </ul>
                <p>Every query also returns a <strong>confidence score</strong>, and every pipeline run is traced via LangSmith for observability and debugging.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TD
    QUERY["User Query"] --> EMBED["1. EMBED<br/>Voyage AI<br/>voyage-multimodal-3.5<br/>(1024-dim)"]
    EMBED --> SEARCH["2. SEARCH<br/>Pinecone Hybrid<br/>(Dense + Sparse BM25)<br/>Namespace isolation"]
    SEARCH --> RERANK["3. RERANK<br/>Jina AI<br/>v2-base-multilingual"]
    RERANK --> COMPRESS["4. COMPRESS<br/>Context compression<br/>Reduce token count"]
    COMPRESS --> GENERATE["5. GENERATE<br/>LLM Router<br/>Answer with citations"]
    GENERATE --> RESPONSE["Answer + Sources<br/>+ Confidence Score"]
    subgraph LLM_CHAIN["LLM Fallback Chain"]
        PRIMARY["Gemini 3 Flash"]
        FALLBACK1["Claude Sonnet 4.5"]
        FALLBACK2["GPT-5.2"]
        PRIMARY -->|fail| FALLBACK1 -->|fail| FALLBACK2
    end
    GENERATE --> LLM_CHAIN
            </pre></div>
            <h3 class="sub-heading">Pinecone Vector Store Structure</h3>
            <p>All document vectors (the numerical representations of meaning) are stored in a single Pinecone index called <code>s42-rag</code>. Within that index, each client's data lives in its own isolated <strong>namespace</strong> — like separate filing cabinets in the same room. When a query runs, it only searches within the relevant client's namespace, ensuring strict data separation. Cross-client search is not possible from a single query.</p>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TD
    INDEX["Pinecone Index: s42-rag"] --> NS1["Namespace: rag-testing<br/>(713 vectors)"]
    INDEX --> NS2["Namespace: client-a<br/>(isolated)"]
    INDEX --> NS3["Namespace: client-b<br/>(isolated)"]
    NS1 --> V1["Vector + Metadata:<br/>source_file, document_type,<br/>client_name, chunk_index"]
    subgraph RULE["API Rule"]
        R1["Single query = ONE namespace only<br/>Cross-namespace = multiple API calls"]
    end
            </pre></div>
        </section>

        <!-- 5 -->
        <section id="data-flow" class="card section-card">
            <div class="section-heading"><span class="section-number">5</span><h2>Full Data Flow — Upload to Answer</h2></div>
            <div class="body">
                <p>This diagram tells the full story in two phases. Read it top to bottom — it shows exactly what happens at each step, which service is responsible, and where data moves.</p>
                <p><strong>Phase 1 — Getting documents in (Ingestion):</strong> A user uploads a document. The file is stored in MinIO (object storage). The Doc-Ingestion Service processes it through the 11-step pipeline, saving checkpoints along the way. Once a human approves the output, the processed chunks are sent to the RAG Engine, which generates vector embeddings (numerical representations of meaning) and stores them in Pinecone for fast retrieval.</p>
                <p><strong>Phase 2 — Answering a question (Query):</strong> A user asks a question. The Memory Service picks up the message, gathers the user's profile, company config, and operational context (calendar, tasks, etc.) from MongoDB, then forwards the question to the RAG Engine. The RAG Engine searches the vector store, re-ranks results, compresses context, sends it to the language model, and returns an answer with source citations. Finally, the Memory Service analyses the conversation to extract useful learnings for next time.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
sequenceDiagram
    participant U as User
    participant DI as Doc-Ingestion :8009
    participant PG as PostgreSQL
    participant S3 as MinIO (S3)
    participant RE as RAG Engine :8008
    participant VA as Voyage AI
    participant PC as Pinecone
    participant JN as Jina AI
    participant LLM as LLM (Gemini/Claude)
    participant MEM as Memory Service :8000
    participant MG as MongoDB
    Note over U,MG: PHASE 1: Document Ingestion
    U->>DI: Upload document
    DI->>S3: Store original file
    DI->>DI: 11-node LangGraph pipeline
    DI->>PG: Save processing state + checkpoints
    U->>DI: Approve document
    DI->>RE: POST /ingest (chunks + metadata)
    RE->>VA: Generate embeddings (1024-dim)
    RE->>PC: Upsert vectors to namespace
    Note over U,MG: PHASE 2: Query &amp; Answer
    U->>MEM: Ask question
    MEM->>MG: Fetch user profile + tenant config
    MEM->>MG: Fetch operational context
    MEM->>MEM: Load service manifest
    MEM->>RE: POST /ask (query + client_id)
    RE->>VA: Embed query
    RE->>PC: Hybrid search (dense + sparse)
    RE->>JN: Rerank results
    RE->>RE: Compress context
    RE->>LLM: Generate answer with sources
    RE->>MEM: Return answer + citations
    MEM->>MEM: LangMem: extract learnings
    MEM->>MG: Store new memories
    MEM->>U: Response with sources
            </pre></div>
        </section>

        <!-- 6 -->
        <section id="infrastructure" class="card section-card">
            <div class="section-heading"><span class="section-number">6</span><h2>Infrastructure &amp; Deployment</h2></div>
            <div class="body">
                <p>All the services and databases described above need somewhere to run. This section shows the <strong>infrastructure layer</strong> — the containers, databases, and deployment targets that make the platform operational.</p>
                <p>In development, the entire platform runs locally via <strong>Docker Compose</strong> — a tool that starts all 10 services (6 application + 4 infrastructure) with a single command. In production, the platform is deployed to <strong>Fly.io</strong>, a cloud hosting provider. External AI services (Pinecone, Voyage AI, Jina AI, LLM providers) are called over the internet rather than being self-hosted.</p>
                <p>The diagram below shows which services depend on which databases. For example, the Doc-Ingestion Service needs PostgreSQL (for processing state), Redis (for the task queue), and MinIO (for file storage). The Memory Service only needs MongoDB. Understanding these dependencies matters when troubleshooting — if PostgreSQL is down, doc-ingestion stops, but the Memory Service and RAG Engine keep working.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TB
    subgraph DOCKER["Docker Compose — signal-core-memory-knowledge"]
        subgraph SVC["Application Services"]
            MEM_SVC["memory :8000"]
            DI_SVC["doc-ingestion :8009"]
            DI_W["celery-worker"]
            DI_FE["doc-ingestion-fe :5173"]
            RAG_SVC["rag-engine :8008"]
            RAG_FE["rag-engine-fe :3008"]
        end
        subgraph INF["Infrastructure"]
            MDB[("MongoDB :27017")]
            PGS[("PostgreSQL :5433")]
            RDS[("Redis :6380")]
            MNO[("MinIO :9000")]
        end
        MEM_SVC --> MDB
        DI_SVC --> PGS
        DI_SVC --> RDS
        DI_SVC --> MNO
        DI_W --> PGS
        DI_W --> RDS
        RAG_SVC --> MNO
    end
    subgraph EXT["External APIs"]
        PC2["Pinecone"]
        VA2["Voyage AI"]
        JN2["Jina AI"]
        LS2["LangSmith"]
        LLM2["LLM Providers"]
    end
    RAG_SVC --> PC2
    RAG_SVC --> VA2
    RAG_SVC --> JN2
    subgraph DEP["Deployment"]
        FLY["Fly.io (Prod)"]
        LOC["Docker Compose (Dev)"]
    end
            </pre></div>
            <h3 class="sub-heading">Port Registry</h3>
            <p>Each service listens on a specific port number — think of ports as numbered doors that let you reach the right service. All ports are defined in <code>docker-compose.yml</code> and are mapped to avoid conflicts with other software that might be running on the same machine.</p>
            <div class="table-wrap">
                <table>
                    <tr><th>Port</th><th>Service</th><th>Type</th><th>Description</th></tr>
                    <tr><td style="font-family:monospace;font-size:13px">3008</td><td>RAG Engine Frontend</td><td><span class="badge badge-ui">UI</span></td><td>Web interface for testing search queries against the RAG Engine and reviewing results.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">5173</td><td>Doc-Ingestion Frontend</td><td><span class="badge badge-ui">UI</span></td><td>Web interface for uploading documents, monitoring pipeline progress, and approving processed content.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">8000</td><td>Memory Service</td><td><span class="badge badge-api">API</span></td><td>The main API that client applications talk to. Handles chat messages, context assembly, and memory management.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">8008</td><td>RAG Engine</td><td><span class="badge badge-api">API</span></td><td>Search and answer API. Called by the Memory Service to find relevant documents and generate answers.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">8009</td><td>Doc-Ingestion</td><td><span class="badge badge-api">API</span></td><td>Document processing API. Receives file uploads and manages the 11-step ingestion pipeline.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">9000</td><td>MinIO API</td><td><span class="badge badge-other">Storage</span></td><td>S3-compatible object storage for uploaded files. Stores original documents and processed outputs.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">9001</td><td>MinIO Console</td><td><span class="badge badge-other">Admin</span></td><td>Web-based admin dashboard for browsing stored files, managing buckets, and monitoring storage usage.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">27017</td><td>MongoDB</td><td><span class="badge badge-db">Database</span></td><td>Document database used by the Memory Service. Stores user profiles, tenant configs, conversation history, and learned memories.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">5433</td><td>PostgreSQL</td><td><span class="badge badge-db">Database</span></td><td>Relational database used by Doc-Ingestion. Stores document metadata, processing state, and pipeline checkpoints.</td></tr>
                    <tr><td style="font-family:monospace;font-size:13px">6380</td><td>Redis</td><td><span class="badge badge-other">Cache/Queue</span></td><td>In-memory data store used as a task queue for batch document processing (Celery workers pick up jobs from here).</td></tr>
                </table>
            </div>
        </section>

        <!-- 7 -->
        <section id="llm-routing" class="card section-card">
            <div class="section-heading"><span class="section-number">7</span><h2>LLM Model Routing</h2></div>
            <div class="body">
                <p>Signal OS doesn't rely on a single AI model — it uses <strong>multiple models from different providers</strong> (Google, Anthropic, OpenAI), each chosen for the task it's best at. The <strong>LLM Router</strong> (<code>packages/rag-engine/backend/app/services/llm_router.py</code>) is the component that makes this selection automatically.</p>
                <p><strong>Why not just use one model for everything?</strong> Because different tasks have different trade-offs. Extracting metadata from a document needs speed and low cost — that's Gemini 3 Flash. Validating whether content is accurate needs precision — that's Claude Sonnet 4.5. By matching tasks to the right model, the system balances quality, speed, and cost.</p>
                <p><strong>Automatic fallback:</strong> If the primary model is unavailable (provider outage, rate limit), the router automatically tries the next model in the chain. This means the platform stays operational even if a single AI provider goes down.</p>
                <p>Each provider also has slightly different configuration requirements (Anthropic uses "temperature", Google requires temperature=1.0 and uses "thinking_level", OpenAI uses "verbosity" and "reasoning_effort"). The router handles all of these differences transparently, so the rest of the codebase doesn't need to know which provider is being used.</p>
            </div>
            <div class="diagram-wrapper" onclick="expandDiagram(this)"><div class="expand-hint">Click to expand</div><pre class="mermaid">
graph TD
    REQ["LLM Request"] --> ROUTER["LLM Router"]
    ROUTER --> TASK{"Task Type"}
    TASK -->|Extraction| G1["Gemini 3 Flash"]
    TASK -->|Validation| C1["Claude Sonnet 4.5"]
    TASK -->|Vision| G2["Gemini 3 Flash"]
    TASK -->|Generation| G3["Gemini 3 Flash"]
    TASK -->|Embeddings| V1["Voyage Multimodal 3.5"]
    TASK -->|Reranking| J1["Jina v2-base"]
    G1 -->|fail| C2["Claude Sonnet 4.5"]
    C1 -->|fail| O1["GPT-5.2"]
    G2 -->|fail| O2["GPT-5.2"]
    G3 -->|fail| C3["Claude Sonnet 4.5"]
    subgraph PARAMS["Provider-Specific Parameters"]
        P_ANT["Anthropic: temperature 0-1"]
        P_OAI["OpenAI: verbosity + reasoning_effort"]
        P_GOO["Google: temperature=1.0 fixed + thinking_level"]
    end
            </pre></div>
        </section>

    </div>

    <footer>
        <p>&copy; 2026 S42 Labs &bull; Part of <a href="https://signal42.co" target="_blank">Signal 42 Group</a></p>
    </footer>

    <!-- Modal -->
    <div class="modal-overlay" id="modal" onclick="if(event.target===this)closeModal()">
        <div class="modal-content">
            <div class="modal-bar">
                <span>Click outside or press Esc to close</span>
                <button class="modal-close" onclick="closeModal()">✕ Close</button>
            </div>
            <div class="modal-body" id="modal-body"></div>
        </div>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                fontSize: '16px',
                fontFamily: 'Inter Tight, system-ui, sans-serif',
                primaryColor: '#6B2FA0',
                primaryTextColor: '#fff',
                primaryBorderColor: '#5A2590',
                lineColor: '#666',
                secondaryColor: '#f5f5f5',
                tertiaryColor: '#fafafa'
            },
            flowchart: { useMaxWidth: true, htmlLabels: true, curve: 'basis', nodeSpacing: 30, rankSpacing: 40, padding: 15 },
            sequence: { useMaxWidth: true, actorFontSize: 15, messageFontSize: 14, noteFontSize: 14, height: 36, mirrorActors: false }
        });

        function expandDiagram(wrapper) {
            var svg = wrapper.querySelector('svg');
            if (svg) {
                document.getElementById('modal-body').innerHTML = svg.outerHTML;
                document.getElementById('modal').classList.add('active');
                document.body.style.overflow = 'hidden';
            }
        }

        function closeModal() {
            document.getElementById('modal').classList.remove('active');
            document.body.style.overflow = '';
        }

        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') closeModal();
        });
    </script>
</body>
</html>
